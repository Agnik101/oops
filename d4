import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data = pd.read_csv('synthetic_user_data.csv')
X = data['Time Spent'].values
y = data['Premium'].values


X = (X - np.mean(X)) / np.std(X)

#part2a>
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

#part2c>
def compute_cost(X, y, theta):
    m = len(y)
    h = sigmoid(np.dot(X, theta)) 
    cost = - (1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) 
    return cost

#minimizing cost function partd>
def gradient_descent(X, y, theta, learning_rate, iterations):
    m = len(y)
    cost_history = []
    
    for i in range(iterations):
        h = sigmoid(np.dot(X, theta))
        gradient = (1/m) * np.dot(X.T, (h - y))  
        theta = theta - learning_rate * gradient 
        cost_history.append(compute_cost(X, y, theta))
        
    return theta, cost_history
X = np.c_[np.ones(len(X)), X]  


theta = np.zeros(X.shape[1]) 


learning_rate = 0.01
iterations = 1000

theta_optimal, cost_history = gradient_descent(X, y, theta, learning_rate, iterations)


b_optimal = theta_optimal[0]
w_optimal = theta_optimal[1] 


plt.plot(range(iterations), cost_history)
plt.xlabel('Iterations')
plt.ylabel('Cost (J)')
plt.title('Cost Function History')
plt.show()


print(f"Optimal w (weight): {w_optimal}")
print(f"Optimal b (bias): {b_optimal}")
